{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "latest-singles",
   "metadata": {},
   "source": [
    "Kaggleì—ì„œ ì œê³µí•˜ëŠ” Crowdflower Search Results Relevance dataë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ë§Œì¡±ë„ íŒë³„ ëª¨ë¸ì„ ê°œë°œí•œ ê³¼ì •ì„ ì •ë¦¬í•´ ë´¤ìŠµë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "daconì—ì„œ ì œê³µí•˜ëŠ” ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì²´ ì¸ì‹ ì•Œê³ ë¦¬ì¦˜ì¸ faster rcnnì„ ê°œë°œí•œ ê³¼ì •ì„ ì •ë¦¬í•´ ë´¤ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-cookbook",
   "metadata": {},
   "source": [
    "ğ‘’32â‹…2ğ‘’2ğœ‹ğ‘– ğ‘‘ğ‘‘ğ‘¥âˆ«ğ‘ğ‘âˆ‘ğ‘–=1ğ‘ğ‘“(ğ‘¥)ğ‘‘ğ‘¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-concern",
   "metadata": {},
   "source": [
    "$$e^\\frac{3}{2} \\cdot 2 e^{2 \\pi i } \\  \\frac{d}{dx}\\int_{a}^{b}\\sum_{i=1}^{N}f(x) dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-aquatic",
   "metadata": {},
   "source": [
    "ë¨¼ì € human joint keypoint detection taskì˜ ë™í–¥ì— ëŒ€í•˜ì—¬ ì•Œì•„ë³´ê¸° ìœ„í•˜ì—¬ ì—­ëŒ€ SOTAë¥¼ ë‹¬ì„±í•œ ë…¼ë¬¸ë“¤ì„ ì½ì–´ ë³´ì•˜ìŠµë‹ˆë‹¤.\n",
    "ê²°ê³¼ë¡œ ì—¬ëŸ¬ê°€ì§€ ë°©ì‹ì¤‘ ì‚¬ëŒì„ ë¨¼ì € detectioní•œë’¤ keypointë¥¼ ì¶”ì •í•˜ëŠ” Top-downë°©ì‹ì´ \n",
    "\n",
    "í•˜ì§€ë§Œ ì•ˆê·¸ë˜ë„ ëŠ¦ê²Œ ì‹œì‘í•˜ì˜€ëŠ”ë° ë…¼ë¬¸ì„ ì½ìœ¼ë©° ê°œë°œ ê³¼ì •ì„ íƒêµ¬ í•˜ëŠ”ë° ì‹œê°„ì„ ë„ˆë¬´ ë§ì´ í—ˆë¹„ í•˜ì—¬,\n",
    "detection modelë§Œì´ë¼ë„ ê°œë°œí•´ë³´ìí•˜ì—¬ faster-rcnnì„ ì ìš©í•˜ê³  ì‹œê°„ì´ ë‚¨ëŠ”ë‹¤ë©´ mask rcnnì„ ì ìš©í•˜ì—¬ key-pointë¥¼ ì¶”ì •í•˜ê¸°ë¡œ ê³„íšì„ ë³€ê²½í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "detection modelì„ ê°œë°œí•´ë³´ëŠ” ê²ƒë§Œìœ¼ë¡œë„ ì¢‹ì€ ê²½í—˜ì´ë¯€ë¡œ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-christian",
   "metadata": {},
   "source": [
    "ì•„ë˜ëŠ” faster rcnn êµ¬í˜„ ì½”ë“œì™€ ì„¤ëª…ì…ë‹ˆë‹¤.\n",
    "ëª¨ë“  ì½”ë“œëŠ” google colabì—ì„œ êµ¬ë™í•˜ì˜€ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-tutorial",
   "metadata": {},
   "source": [
    "backboneì€ resnet50ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "ë˜í•œ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ í¬ê¸° ë•Œë¬¸ì— 0.4ë°° í•˜ì—¬ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-warrior",
   "metadata": {},
   "source": [
    "ë¨¼ì € daconì—ì„œ ì œê³µí•˜ëŠ” dataëŠ” key point ì¶”ì •ì´ë¯€ë¡œ bounding boxì— ëŒ€í•œ ë¼ë²¨ì€ì—†ìŠµë‹ˆë‹¤.\n",
    "ë”°ë¼ì„œ ê° key pointì—ì„œ x, yì¶• ìµœëŒ€, ìµœì†Œ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œx, yì¶• ìµœëŒ€, ìµœì†Œ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ bounding boxë¥¼ ì„¤ì •í•˜ê¸°ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_generator(target):\n",
    "    x_min = np.min(target[::2])\n",
    "    x_max = np.max(target[::2])\n",
    "    y_min = np.min(target[1::2])\n",
    "    y_max = np.max(target[1::2])\n",
    "    ratio = 'w' if x_max - x_min > y_max - y_min else 'h'\n",
    "\n",
    "    x_min = x_min - (x_max - x_min)*.1 if ratio == 'h' else x_min\n",
    "    x_max = x_max + (x_max - x_min)*.1 if ratio == 'h' else x_max\n",
    "    y_min = y_min - (y_max - y_min)*.1 if ratio == 'w' else y_min\n",
    "    y_max = y_max + (y_max - y_min)*.1 if ratio == 'w' else y_max\n",
    "\n",
    "    ground_truth_x_min = x_min - (x_max - x_min)*.05\n",
    "    ground_truth_x_max = x_max + (x_max - x_min)*.05\n",
    "    ground_truth_y_min = y_min - (y_max - y_min)*.05\n",
    "    ground_truth_y_max = y_max + (y_max - y_min)*.05\n",
    "\n",
    "    ground_truth_w = ground_truth_x_max - ground_truth_x_min\n",
    "    ground_truth_h = ground_truth_y_max - ground_truth_y_min\n",
    "    ground_truth_x = ground_truth_w/2 + ground_truth_x_min\n",
    "    ground_truth_y = ground_truth_h/2 + ground_truth_y_min\n",
    "    return [ground_truth_x, ground_truth_y, ground_truth_w, ground_truth_h]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-mountain",
   "metadata": {},
   "source": [
    "key pointì—ì„œ y ìµœì†Œê°’ì€ ëˆˆ, íŒ”ì„ ë“¤ê³  ìˆë‹¤ë©´ ì†ëª©, ìµœëŒ“ê°’ì€ ë°œëª©, ë“±\n",
    "x ìµœì†Ÿê°’ì€ ì†ëª©, ëˆ„ì›ŒìˆëŠ”ê²½ìš° ëˆˆ, ë°œëª© ë“±ìœ¼ë¡œ \n",
    "ë‹¨ìˆœíˆ ìµœëŒ€ ìµœì†Œ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ bounding boxë¥¼ ì„¤ì •í•˜ë©´ ì˜ë¦¬ê¸° ë•Œë¬¸ì— ê°€ë¡œ ì„¸ë¡œë¥¼ í™•ì¥í•˜ê¸°ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "ë˜í•œ w, hì˜ ë¹„ìœ¨ì„ ìƒê°í•˜ì—¬ wê°€ ê¸¸ë©´ ëˆ„ì›ŒìˆëŠ” ê²½ìš°, hê°€ ê¸¸ë©´ ì„œ ìˆëŠ” ê²½ìš°ì„ì„ í™•ì¸ í•˜ì˜€ìŠµë‹ˆë‹¤. \n",
    "ë¹„ìœ¨ì´ ë¹„ìŠ·í•œ ê²½ìš°ëŠ” ìƒì²´ë¥¼ ìˆ™ì´ê³  ìˆê±°ë‚˜ ë²¤ì¹˜ì— ëˆ„ì›Œì„œ íŒ”ì„ ë“¤ì–´ì˜¬ë¦° ê²½ìš°ê°€ ìˆì§€ë§Œ ìœ„ì™€ ê°™ì´ ì²˜ë¦¬í•˜ì˜€ì„ë•Œ ì‚¬ëŒì„ ì˜ í¬í•¨í•˜ê¸° ë•Œë¬¸ì— ì´ ê²½ìš°ëŠ” ë”°ë¡œ ì²˜ë¦¬ í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì¦‰, ì„œìˆëŠ”ê²½ìš°ì™€ ëˆ„ì›ŒìˆëŠ” ê²½ìš°ì˜ ê°€ë¡œ ì„¸ë¡œ í™•ì¥ì„ ë‹¤ë¥´ê²Œ ë‘ì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-injection",
   "metadata": {},
   "source": [
    "bounding boxë¥¼ ì„¤ì •í•˜ì˜€ë‹¤ë©´ anchor boxë¥¼ ì„¤ì •í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤. \n",
    "ë¨¼ì € ì•„ë˜ì˜ ê¸°ì¤€ì„ ìƒˆìš°ê³  ì´ë¥¼ ê·¸ë˜í”„ë¡œ ê·¸ë ¤ í™•ì¸í•˜ê³  ê²°ê³¼ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ê³¼ì •ì„ ê±°ì³\n",
    "\n",
    "ë¨¼ì € ì•„ë˜ì˜ ê¸°ì¤€ìœ¼ë¡œ ground truth bounding boxì˜ ë¶„í¬ë¥¼ ê·¸ë˜í”„ ëˆˆìœ¼ë¡œ í™•ì¸í•˜ì—¬ anchor boxë¥¼ ì–´ë–»ê²Œ ì„¤ì •í• ì§€ ë³´ì•„ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "1. ground truth bounding boxì˜ ë„“ì´ ë¶„í¬\n",
    "2. w, hì˜ ê¸¸ì´ ë¹„\n",
    "3. anchorì˜ ê°¯ìˆ˜\n",
    "\n",
    "ë„“ì´ëŠ” 140, 160, 180, 210, 240\n",
    "w, hì˜ ë¹„ìœ¨ì€ ë™ì¼í•œ ê²½ìš°, 2ë°°, 3ë°°ë¡œ êµ¬ë¶„í•˜ì—¬ ì„¤ì •í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "ë”°ë¼ì„œ anchorì˜ ê°¯ìˆ˜ëŠ” bin í•˜ë‚˜ë‹¹ 5 x 5 ê°œê°€ ìƒì„± ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "medical-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_box_generator(x, y, scales, ratio):\n",
    "    anchor_boxes = []\n",
    "    for scale in scales:\n",
    "        for w, h  in ratio:\n",
    "            w *= scale\n",
    "            h *= scale     \n",
    "            anchor_boxes.append([x, y, w, h])\n",
    "    return anchor_boxes\n",
    "\n",
    "def Anchor_Boxes(img_shape, scales, ratio, model='vgg'):\n",
    "    '''\n",
    "    input\n",
    "    img_shape : image shape\n",
    "    output \n",
    "    numpy array shape (w * h * k, 4)\n",
    "    '''\n",
    "    if model == 'vgg':\n",
    "        Ratio = 2**4\n",
    "        \n",
    "    w=img_shape[1]//Ratio\n",
    "    h=img_shape[0]//Ratio\n",
    "    \n",
    "    anchor_boxes = []\n",
    "    for x in range(img_shape[1]//w//2, img_shape[1], img_shape[1]//w):\n",
    "        for y in range(img_shape[0]//h//2, img_shape[0], img_shape[0]//h):\n",
    "            anchor_boxes.append(anchor_box_generator(x, y, scales, ratio))\n",
    "    return np.array(anchor_boxes).reshape(-1, 4)\n",
    "\n",
    "scales = [140, 160, 180, 210, 240]\n",
    "ratio = [(1/np.sqrt(3), np.sqrt(3)), \n",
    "         (1/np.sqrt(2), np.sqrt(2)), \n",
    "         (1, 1), \n",
    "         (np.sqrt(2), 1/np.sqrt(2)), \n",
    "         (np.sqrt(3), 1/np.sqrt(3))]\n",
    "anchor_boxes = Anchor_Boxes((432, 768, 3), scales, ratio, model='vgg')\n",
    "\n",
    "bboxes = anchors_to_coordinates(anchor_boxes)\n",
    "out_boundaries_indxes = (np.where(bboxes[:, 0] < 0) or np.where(bboxes[:, 2] < 0) or \n",
    "                         np.where(bboxes[:, 1] > 768) or np.where(bboxes[:, 3] > 432))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conservative-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import anchors_to_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-terrace",
   "metadata": {},
   "source": [
    "anchor boxë¥¼ ë³´ë©´ ì´ë¯¸ì§€ì˜ ë²”ìœ„ë¥¼ ë„˜ëŠ” anchorê°€ ìƒë‹¹ìˆ˜ ì¡´ì¬í•©ë‹ˆë‹¤. í•™ìŠµê³¼ì •ì—ì„œ ì´ë¥¼ ë¬´ì‹œ í•˜ê¸° ìœ„í•´ì„œ out_boundaries_indxesë¥¼ ë”°ë¡œ ê³„ì‚°í•˜ì—¬ labelì„ ìƒì„±í• ë•Œ ë¬´ì‹œí•˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-republic",
   "metadata": {},
   "source": [
    "anchor boxë¥¼ ì„¤ì •í•˜ì˜€ìœ¼ë‹ˆ ground truthì™€ ë¹„êµí•˜ì—¬ labelì„ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "iouê°€ ê° anchor boxë“¤ê³¼ ground truthì˜ iouê°€ 0.7ì´ìƒì¸ boxë¥¼ P, 0.3 ì´í•˜ì¸ boxë¥¼ Nìœ¼ë¡œ ë‘ê³  ë‚˜ë¨¸ì§€ëŠ” ê³ ë ¤ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ë•Œ Pì™€ Nì˜ ë¹„ìœ¨ì´ 1:1ì´ ë˜ë„ë¡ ì„¤ì •í•˜ê¸° ìœ„í•˜ì—¬ ì´ë¯¸ì§€ë‹¹ label ê°¯ìˆ˜ë¥¼ ì„¤ì • 32ë¡œ ì„¤ì • í•˜ì˜€ìŠµë‹ˆë‹¤. \n",
    "ê²€ì¶œí•˜ì—¬ì•¼ í•˜ëŠ” ì‚¬ëŒì´ ì´ë¯¸ì§€ë‹¹ ë‹¨ í•˜ë‚˜ì´ë©°, ìì„¸ê°€ ë‹¤ì–‘í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ê°¯ìˆ˜ë¥¼ 32ë¡œ ì„¤ì • í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì¶”ê°€ë¡œ bounding box regressionì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•˜ì—¬ ì¢Œí‘œë¥¼ ë…¼ë¬¸ì—ì„œ ì†Œê°œëœ í•¨ìˆ˜ë¥¼ í†µê³¼ ì‹œì¼œì£¼ì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_generator(GT, anchor_boxes, out_boundaries_indxes):\n",
    "    cls_label = - np.ones(shape=(anchor_boxes.shape[0]))\n",
    "    pos_iou_threshold = 0.7\n",
    "    neg_iou_threshold = 0.3\n",
    "    n_sample = 32\n",
    "    pos_ratio = 0.5\n",
    "    n_pos = int(pos_ratio * n_sample)\n",
    "    \n",
    "    ious = np.apply_along_axis(IoU_np, 0, GT, anchor_boxes=anchor_boxes)\n",
    "    cls_label[ious >= pos_iou_threshold] = 1\n",
    "    cls_label[ious < neg_iou_threshold] = 0\n",
    "    cls_label[np.argmax(ious)] = 1\n",
    "    cls_label[out_boundaries_indxes] = -1\n",
    "\n",
    "    pos_index = np.where(cls_label == 1)[0]\n",
    "    if len(pos_index) > n_pos:\n",
    "        disable_index = np.random.choice(\n",
    "            pos_index,\n",
    "            size = (len(pos_index) - n_pos),\n",
    "            replace=False\n",
    "        )\n",
    "        cls_label[disable_index] = -1\n",
    "\n",
    "    n_neg = n_sample - np.sum(cls_label == 1)\n",
    "    neg_index = np.where(cls_label == 0)[0]\n",
    "    if len(neg_index) > n_neg:\n",
    "        disable_index = np.random.choice(\n",
    "            neg_index, \n",
    "            size = (len(neg_index) - n_neg),             \n",
    "            replace = False\n",
    "        )\n",
    "        cls_label[disable_index] = -1\n",
    "\n",
    "    reg_label = anchor_boxes * np.broadcast_to(tf.cast(cls_label > 0, tf.int32), (4, len(cls_label))).T\n",
    "    indices = np.where(reg_label != 0)[0][::4]\n",
    "    x, y, w, h = GT[0], GT[1], GT[2], GT[3]\n",
    "\n",
    "    tx = (x - reg_label[indices][:, 0]) / (reg_label[indices][:, 2])\n",
    "    ty = (y - reg_label[indices][:, 1]) / (reg_label[indices][:, 3])\n",
    "    tw = np.log(w / reg_label[indices][:, 2]) \n",
    "    th = np.log(h / reg_label[indices][:, 3]) \n",
    "    reg_label[indices] = np.stack([tx, ty, tw, th]).T\n",
    "\n",
    "    return cls_label, reg_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-august",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "presidential-burst",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "corrected-asthma",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "manual-bobby",
   "metadata": {},
   "source": [
    "ë‹¤ìŒì€ date augmentationì„ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œ ì…ë‹ˆë‹¤. ì´ëŠ” daconì— ì½”ë“œë¥¼ ê³µìœ í•˜ì—¬ ì£¼ì‹  ë¶„ì´ ê³„ì…”ì„œ ì¡°ê¸ˆ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš© í•˜ì˜€ìŠµë‹ˆë‹¤.  \n",
    "ì•„ë˜ ë§í¬ì—ì„œ í™•ì¸ í•´ì£¼ì„¸ìš”   \n",
    "[date augmentation](https://dacon.io/competitions/official/235701/codeshare/2383?page=2&dtype=recent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traingtGenerator():\n",
    "    Rx, Ry = 0.4, 0.4\n",
    "    image_size = (1080, 1920, 3)\n",
    "    size = [int(image_size[0] * Rx), int(image_size[1] * Ry)]\n",
    "    iter_num = len(train)\n",
    "\n",
    "    for i in range(iter_num):\n",
    "        img = tf.io.read_file(train_val_dir + 'train/' + train['image'].iloc[i]) \n",
    "        img = tf.image.decode_jpeg(img, channels=3) \n",
    "        img = tf.image.resize(img, size) \n",
    "        img = img/255                         \n",
    "        target = list(train.iloc[:,1:49].iloc[i,:])\n",
    "        gt = gt_generator(target)\n",
    "        cls_label, reg_label = label_generator(gt, anchor_boxes, out_boundaries_indxes)\n",
    "\n",
    "        yield img, (cls_label, reg_label, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    traingtGenerator,\n",
    "    output_signature = (\n",
    "            tf.TensorSpec(shape=(size[0], size[1], 3)),\n",
    "            (\n",
    "                tf.TensorSpec(shape=(len(anchor_boxes))),\n",
    "                tf.TensorSpec(shape=(len(anchor_boxes),4)),\n",
    "                tf.TensorSpec(shape=(4))\n",
    "            )\n",
    "        )\n",
    ").batch(batch_size).prefetch(batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-graduation",
   "metadata": {},
   "source": [
    "ë°ì´í„° ë¡œë“œëŠ” data í•¨ìˆ˜ë¥¼ í†µí•˜ì—¬ ëª¨ë¸ì˜ inputìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ í•˜ì˜€ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "embedded-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base(img_size, model='resnet50'):\n",
    "    if model=='vgg':\n",
    "        base_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=img_size)\n",
    "        feature_extractor = base_model.get_layer(\"block5_conv3\")\n",
    "    elif model == 'resnet101':\n",
    "        base_model = tf.keras.applications.ResNet101(include_top=False, weights='imagenet', input_shape=img_size)\n",
    "        feature_extractor = base_model.get_layer(\"conv4_block23_out\")\n",
    "    elif model == 'resnet50':\n",
    "        base_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=img_size)\n",
    "        feature_extractor = base_model.get_layer(\"conv4_block6_out\")  \n",
    "    else:\n",
    "        raise Exception('vgg, resnet')\n",
    "\n",
    "    base_model = tf.keras.models.Model(inputs=base_model.input, outputs=feature_extractor.output)\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "useful-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(tf.keras.models.Model):\n",
    "    def __init__(self, img_size, anchor_boxes, k=5*5, n_sample=32, backbone='resnet50', rpn_lambda=10**3, **kwargs):\n",
    "        super(RPN, self).__init__(**kwargs)\n",
    "        self.img_size = img_size\n",
    "        self.anchor_boxes = anchor_boxes\n",
    "        self.num_of_anchor = len(self.anchor_boxes)\n",
    "        self.n_sample = n_sample\n",
    "        self.k = k\n",
    "        self.backbone = backbone\n",
    "        self.rpn_lambda = rpn_lambda\n",
    "\n",
    "        self.base_model = get_base(self.img_size, model=self.backbone)\n",
    "        self.window = tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='same', name='window')\n",
    "        self.window_bn = tf.keras.layers.BatchNormalization(name='window_bn')\n",
    "        self.window_relu = tf.keras.layers.ReLU(name='window_relu')\n",
    "\n",
    "        self.bbox_reg = tf.keras.layers.Conv2D(filters=self.k*4, kernel_size=1, activation='relu', name='bbox_reg')\n",
    "        self.bbox_reg_reshape = tf.keras.layers.Reshape((-1, 4), name='reg_out')\n",
    "        \n",
    "        self.cls = tf.keras.layers.Conv2D(filters=self.k, kernel_size=1, activation='sigmoid', name='cls')\n",
    "        self.cls_reshape = tf.keras.layers.Reshape((-1, 1), name='cls_out')\n",
    "\n",
    "    def compile(self, optimizer, **kwargs):\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "        self.test_loss_tracker = tf.keras.metrics.Mean(name='test_loss')\n",
    "        self.optimizer = optimizer\n",
    "        super(RPN, self).compile(**kwargs)\n",
    "    \n",
    "    def Cls_Loss(self, y_true, y_pred):\n",
    "        indices = tf.where(tf.not_equal(y_true, tf.constant(-1.0, dtype=tf.float32)))\n",
    "        target = tf.gather_nd(y_true, indices)\n",
    "        output = tf.gather_nd(y_pred, indices)\n",
    "        return tf.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.SUM)(target, output)/self.n_sample\n",
    "\n",
    "    def Reg_Loss(self, y_true, y_pred):\n",
    "        indices = tf.reduce_any(tf.not_equal(y_true, 0), axis=-1)\n",
    "        return tf.losses.Huber(reduction=tf.losses.Reduction.SUM)(y_true[indices], y_pred[indices])/self.num_of_anchor\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        y_cls = y[0]\n",
    "        y_reg = y[1]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            cls, bbox_reg, _ = self(x, training=True)\n",
    "            cls_loss = self.Cls_Loss(y_cls, cls)\n",
    "            reg_loss = self.Reg_Loss(y_reg, bbox_reg)\n",
    "            losses = cls_loss + self.rpn_lambda * reg_loss\n",
    "            \n",
    "        trainable_vars = self.trainable_variables\n",
    "        grad = tape.gradient(losses, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(grad, trainable_vars))\n",
    "        self.loss_tracker.update_state(losses)\n",
    "        return {'rpn_loss': self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_cls = y[0]\n",
    "        y_reg = y[1]\n",
    "        \n",
    "        cls, bbox_reg, _ = self(x, training=False)\n",
    "        cls_loss = self.Cls_Loss(y_cls, cls)\n",
    "        reg_loss = self.Reg_Loss(y_reg, bbox_reg)\n",
    "        losses = cls_loss + self.rpn_lambda * reg_loss\n",
    "\n",
    "        self.test_loss_tracker.update_state(losses)\n",
    "        return {'rpn_loss_val': self.test_loss_tracker.result()}\n",
    "\n",
    "    @tf.function\n",
    "    def bbox_regression(self, boxes):\n",
    "        tx = (boxes[:, :, 0] - self.anchor_boxes[:, 0]) / self.anchor_boxes[:, 2]\n",
    "        ty = (boxes[:, :, 1] - self.anchor_boxes[:, 1]) / self.anchor_boxes[:, 3]\n",
    "        tw = tf.math.log(tf.maximum(boxes[:, :, 2], np.finfo(np.float64).eps) / self.anchor_boxes[:, 2])\n",
    "        th = tf.math.log(tf.maximum(boxes[:, :, 3], np.finfo(np.float64).eps) / self.anchor_boxes[:, 3])\n",
    "        return tf.stack([tx, ty, tw, th], -1)\n",
    "\n",
    "    @tf.function\n",
    "    def inverse_bbox_regression(self, boxes):\n",
    "        gx = self.anchor_boxes[:, 2] * boxes[:, :, 0] + self.anchor_boxes[:, 0]\n",
    "        gy = self.anchor_boxes[:, 3] * boxes[:, :, 1] + self.anchor_boxes[:, 1]\n",
    "        gw = self.anchor_boxes[:, 2] * tf.exp(boxes[:, :, 2])\n",
    "        gh = self.anchor_boxes[:, 3] * tf.exp(boxes[:, :, 3])\n",
    "        return tf.stack([gx, gy, gw, gh], axis=-1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        feature_extractor = self.base_model(inputs)\n",
    "        intermediate = self.window(feature_extractor)\n",
    "        intermediate = self.window_bn(intermediate)\n",
    "        intermediate = self.window_relu(intermediate)\n",
    "\n",
    "        cls = self.cls(intermediate)\n",
    "        cls = self.cls_reshape(cls)\n",
    "        bbox_reg = self.bbox_reg(intermediate)\n",
    "        bbox_reg = self.bbox_reg_reshape(bbox_reg)\n",
    "        bbox_reg = self.bbox_regression(bbox_reg)\n",
    "        return cls, bbox_reg, feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-johnson",
   "metadata": {},
   "source": [
    "faster rcnnì˜ region proposal network êµ¬í˜„ì…ë‹ˆë‹¤.\n",
    "get_baseë¡œ imagenetì—ì„œ pre-trainëœ resnet50ì„ ë¶ˆëŸ¬ì˜¤ê³  \n",
    "ê° anchor boxì—ì„œ objectê°€ ì¡´ì¬í•˜ëŠ”ì§€ íŒë‹¨í•˜ëŠ” cls branchì™€ objectê°€ ì¡´ì¬í•œë‹¤ë©´ bounbingë°•ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” bbox_reg branchë¥¼ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "lossëŠ” ë…¼ë¬¸ì—ì„œ ì†Œê°œí•œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì˜€ìœ¼ë©° \n",
    "clsì˜ ê²½ìš° ìœ„ì—ì„œ -1ë¡œ ì„¤ì •í•œ anchorëŠ” ë¬´ì‹œí•˜ê³  ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "bbox regì˜ ê²½ìš° labelì„ ë§Œë“¤ë•Œ iouê°€ 0.7 ì´ìƒì¸ ê°’ì—ì„œë§Œ ê³„ì‚° í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë˜í•œ rpn_lambdaëŠ” ì´ë¯¸ì§€ë‹¹ labelì˜ ê°¯ìˆ˜ì¸ n_sampleê³¼ anchor boxì˜ ê°¯ìˆ˜ì¸ num_of_anchorì˜ ë¹„ìœ¨ì¸ 10\\*\\*3ìœ¼ë¡œ ì„¤ì • í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "box regressionì—ì„œ relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_candidate_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(get_candidate_layer, self).__init__(**kwargs)\n",
    "\n",
    "    def anchors_clip(self, boxes, size=(432, 768)):    \n",
    "        x1 = boxes[:, :, 0] - boxes[:, :, 2]/2\n",
    "        x2 = boxes[:, :, 0] + boxes[:, :, 2]/2\n",
    "        y1 = boxes[:, :, 1] - boxes[:, :, 3]/2\n",
    "        y2 = boxes[:, :, 1] + boxes[:, :, 3]/2\n",
    "        \n",
    "        x1 = tf.clip_by_value(x1, 0, size[1])\n",
    "        x2 = tf.clip_by_value(x2, 0, size[1])\n",
    "        y1 = tf.clip_by_value(y1, 0, size[0])\n",
    "        y2 = tf.clip_by_value(y2, 0, size[0])\n",
    "\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "        x = x1 + w/2\n",
    "        y = y1 + h/2\n",
    "        return tf.stack([x, y, w, h], axis=-1)\n",
    "\n",
    "    def call(self, x):\n",
    "        scores, rps, n_pre_nms = x\n",
    "        rois = self.anchors_clip(rps)\n",
    "\n",
    "        oobw = tf.expand_dims(tf.cast(tf.math.greater(rois[:, :, 2], 16), tf.float32), -1)\n",
    "        oobh = tf.expand_dims(tf.cast(tf.math.greater(rois[:, :, 3], 16), tf.float32), -1)\n",
    "        scores = tf.math.multiply(scores, oobw)\n",
    "        scores = tf.math.multiply(scores, oobh)\n",
    "\n",
    "        orders = tf.argsort(scores, direction='DESCENDING', axis=1)[:, :n_pre_nms]\n",
    "        rois = tf.gather_nd(rois, orders, batch_dims=1)\n",
    "        scores = tf.gather_nd(scores, orders, batch_dims=1)\n",
    "        return rois, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-month",
   "metadata": {},
   "source": [
    "rpnì„ í†µê³¼í•˜ì—¬ ë‚˜ì˜¨ boxë“¤ì„ ëª¨ë‘ ì‚¬ìš©í•˜ì§€ ì•Šê³  get_candidate_layerí†µí•´ ì•„ë˜ì˜ ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤.\n",
    "1. x, y ì¢Œí‘œê°€ ì´ë¯¸ì§€ì˜ ì‚¬ì´ì¦ˆë¥¼ ë„˜ì–´ê°€ëŠ” ê²½ìš° clip\n",
    "2. ê°€ë¡œ, ì„¸ë¡œì˜ ê¸¸ì´ê°€ 16 ì´í•˜ì¸ boxë“¤ ì œì™¸\n",
    "3. confidence scoreê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í›„ ìƒìœ„ n_pre_nmsê°œì˜ boxë“¤ë§Œ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMS(tf.keras.layers.Layer):\n",
    "    def __init__(self, iou_threshold=0.7, **kwargs):\n",
    "        self.iou_threshold = iou_threshold\n",
    "        super(NMS, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        rois, scores, max_output_size = inputs\n",
    "        selected_indices_padded = tf.image.non_max_suppression_padded(\n",
    "            rois, \n",
    "            tf.squeeze(scores), \n",
    "            max_output_size=max_output_size,\n",
    "            iou_threshold=0.7,\n",
    "            pad_to_max_output_size=True\n",
    "        )[0]\n",
    "        nms = tf.gather(rois, selected_indices_padded, batch_dims=1)\n",
    "        return nms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-migration",
   "metadata": {},
   "source": [
    "get_candidate_layerë¥¼ í†µê³¼í•˜ì—¬ ë‚˜ì˜¨ í›„ë³´ ì§€ì—­ë“¤ì„ non maximum suppressionì„ í†µí•˜ì—¬ ë‹¤ì‹œ max_output_sizeê°œë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoIpool(tf.keras.layers.Layer):\n",
    "    def __init__(self, pool_size=7, **kwargs):\n",
    "        self.pool_size = pool_size\n",
    "        super(RoIpool, self).__init__(**kwargs)\n",
    "\n",
    "    def cal_rois_ratio(self, nmses, size=[432, 768]):\n",
    "        x1 = (nmses[:, :, 0] - nmses[:, :, 2]/2)/size[1]\n",
    "        x2 = (nmses[:, :, 0] + nmses[:, :, 2]/2)/size[1]\n",
    "        y1 = (nmses[:, :, 1] - nmses[:, :, 3]/2)/size[0]\n",
    "        y2 = (nmses[:, :, 1] + nmses[:, :, 3]/2)/size[0]\n",
    "        return tf.stack([y1, x1, y2, x2], axis=-1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        feature_map, nmses = inputs\n",
    "        n_channel = feature_map.shape[-1]\n",
    "        batch_size = nmses.shape[0]\n",
    "        num_rois = nmses.shape[1]\n",
    "        nmses = self.cal_rois_ratio(nmses)\n",
    "        rois = tf.image.crop_and_resize(\n",
    "            feature_map, \n",
    "            tf.reshape(nmses, (-1, 4)), \n",
    "            box_indices=tf.convert_to_tensor([i for i in range(batch_size) for _ in range(num_rois)]), \n",
    "            crop_size=[self.pool_size, self.pool_size]\n",
    "        )\n",
    "        return tf.reshape(rois, shape=(batch_size, num_rois, self.pool_size, self.pool_size, n_channel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-arbitration",
   "metadata": {},
   "source": [
    "NMSë¥¼ í†µí•´ ë‚˜ì˜¨ ì§€ì—­ë“¤ì€ classifierì˜ inputsìœ¼ë¡œ ì…ë ¥ë˜ê¸° ìœ„í•´ roi poolingì„ í•©ë‹ˆë‹¤. \n",
    "mask rcnnêµ¬í˜„ì„ ì°¾ì•„ë³´ë˜ì¤‘ tf.image.crop_and_resizeí•¨ìˆ˜ê°€ roi alignì˜ ìˆ˜í–‰ê³¼ ê°™ì€ processë¼ê³  í•˜ì—¬ ê°„ë‹¨íˆ êµ¬í˜„ í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-telephone",
   "metadata": {},
   "source": [
    "classifierë¥¼ êµ¬í˜„í•˜ê¸° ì „ì— ë‹¤ì‹œ í•œë²ˆ label generatorë¥¼ êµ¬í˜„í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(rois, gts):\n",
    "    box1_area = tf.cast(rois[:, :, 2] * rois[:, :, 3], tf.float64)\n",
    "    box2_area = tf.cast(gts[:, 2] * gts[:, 3], tf.float64)\n",
    "    \n",
    "    x1 = tf.maximum(tf.cast(rois[:, :, 0] - rois[:, :, 2]/2, tf.float64), tf.cast(tf.expand_dims(gts[:, 0] - gts[:, 2]/2, -1), tf.float64))\n",
    "    x2 = tf.minimum(tf.cast(rois[:, :, 0] + rois[:, :, 2]/2, tf.float64), tf.cast(tf.expand_dims(gts[:, 0] + gts[:, 2]/2, -1), tf.float64))\n",
    "    y1 = tf.maximum(tf.cast(rois[:, :, 1] - rois[:, :, 3]/2, tf.float64), tf.cast(tf.expand_dims(gts[:, 1] - gts[:, 3]/2, -1), tf.float64))\n",
    "    y2 = tf.minimum(tf.cast(rois[:, :, 1] + rois[:, :, 3]/2, tf.float64), tf.cast(tf.expand_dims(gts[:, 1] + gts[:, 3]/2, -1), tf.float64))\n",
    "    \n",
    "    h = tf.maximum(tf.constant(0.0, dtype=tf.float64), y2 - y1)\n",
    "    w = tf.maximum(tf.constant(0.0, dtype=tf.float64), x2 - x1)\n",
    "    \n",
    "    intersect = tf.math.multiply(h, w)\n",
    "    union = tf.subtract(tf.add(box1_area, tf.expand_dims(box2_area, -1)), intersect) + 1e-12\n",
    "    return tf.divide(intersect, union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_label_generator(nms, gts, valid=False):\n",
    "    num_roi = 128\n",
    "    pos_ratio = .5\n",
    "    num_pos = int(num_roi * pos_ratio)\n",
    "    num_neg = int(num_roi - num_pos)\n",
    "    ious = get_iou(nms, gts)\n",
    "    pos_order = tf.argsort(ious * tf.cast(tf.math.greater_equal(ious, 0.5), tf.float64), direction='DESCENDING', axis=1)[:, :num_pos]\n",
    "    neg_order = tf.argsort(ious * tf.cast(tf.math.less(ious, 0.5), tf.float64), direction='DESCENDING', axis=1)[:, :num_neg]\n",
    "    \n",
    "    if not valid:\n",
    "        neg_cnt = tf.reduce_min(tf.math.count_nonzero(ious * tf.cast(tf.math.less(ious, 0.5), tf.float64), axis=1))\n",
    "        neg_order = tf.argsort(ious * tf.cast(tf.math.less(ious, 0.5), tf.float64), direction='DESCENDING', axis=1)[:, :neg_cnt]\n",
    "        indices = tf.range(start=0, limit=neg_cnt, dtype=tf.int32)\n",
    "        shuffled_indices = tf.random.shuffle(indices)\n",
    "        neg_order = tf.gather(neg_order, shuffled_indices, axis=1)[:, :num_neg]\n",
    "\n",
    "    cls_labels = tf.concat([tf.ones_like(pos_order), tf.zeros_like(neg_order)], axis=1)\n",
    "    label_order = tf.concat([pos_order, neg_order], axis=1)\n",
    "    P_boxes = tf.gather(nms, label_order, batch_dims=1)\n",
    "    \n",
    "    tx = (gts[:, :1] - P_boxes[:, :, 0]) / P_boxes[:, :, 2]\n",
    "    ty = (gts[:, 1:2] - P_boxes[:, :, 1]) / P_boxes[:, :, 3]\n",
    "    tw = tf.math.log(gts[:, 2:3] / P_boxes[:, :, 2]) \n",
    "    th = tf.math.log(gts[:, 3:] / P_boxes[:, :, 3]) \n",
    "    tf.stack([tx, ty, tw, th], axis=-1)\n",
    "    box_labels = tf.stack([tx, ty, tw, th], axis=-1)\n",
    "    return box_labels, cls_labels, P_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-commission",
   "metadata": {},
   "source": [
    "classifierì—ì„œ labelì€ ê° objectë³„ë¡œ confidence scoreì™€ bounding box regressionì„ ìˆ˜í–‰í•˜ëŠ”ë° ì£¼ì–´ì§„ ë°ì´í„°ì™€ taskëŠ” ë‹¨ í•œëª…ì˜ ì‚¬ëŒë§Œ ê²€ì¶œí•˜ë©´ ë˜ë¯€ë¡œ rpnê³¼ ë¹„ìŠ·í•œ ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤. \n",
    "\n",
    "ë‹¤ë¥¸ì ì€ classiferì—ì„œëŠ” ì¶”ì¶œëœ ì§€ì—­ê³¼ ground truthì˜ iouê°€ 0.5ì´ìƒì¸ ê°’ì´ Pì´ê³  ë‚˜ë¨¸ì§€ê°€ Nì´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì—¬ê¸°ì„œ ê° ì´ë¯¸ì§€ë‹¹ 128ê°œì˜ labelì„ ì„¤ì •í•˜ëŠ”ë° ë…¼ë¬¸ì—ì„œëŠ” 25% Pë¥¼ í•˜ì˜€ì§€ë§Œ ì €ëŠ” 50% ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤. \n",
    "ì´ëŠ” ë‹¨! í•œëª…ì˜ ì‚¬ëŒë§Œ ê²€ì¶œí•˜ëŠ” ê³¼ì • + dataìƒì—ì„œ íŠ¹ì • ìœ„ì¹˜ì— ì‚¬ëŒì´ ìˆëŠ”ê²½ìš°ê°€ ë§ì•„ classifierê°€ ì¡°ê¸ˆë” ë‹¤ì–‘í•œ ìœ„ì¹˜ë¥¼ í¬ì°©í•˜ê²Œ í•˜ê¸° ìœ„í•´ ì¡°ì¹˜ í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë§ˆì°¬ê°€ì§€ë¡œ bounding box regressionê³¼ì •ë„ ê±°ì¹©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-assessment",
   "metadata": {},
   "source": [
    "$$ \\int_{a}^{b}f(x) dx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-developer",
   "metadata": {},
   "source": [
    "$$a \\sim b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-trash",
   "metadata": {},
   "source": [
    "$$a \\sum_{i=1}^{N} b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(tf.keras.models.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Classifier, self).__init__(**kwargs)\n",
    "        self.conv = tf.keras.layers.Conv2D(2048, kernel_size=(7, 7), name='conv')\n",
    "        self.bn = tf.keras.layers.BatchNormalization(name='conv_bn')\n",
    "        self.relu = tf.keras.layers.ReLU(name='conv_relu')\n",
    "        self.flatten = tf.keras.layers.Flatten(name='flatten')\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(1024, name='dense1')\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization(name='dense1_bn')\n",
    "        self.relu1 = tf.keras.layers.ReLU(name='dense1_relu')\n",
    "\n",
    "        self.dense2 = tf.keras.layers.Dense(1024, name='dense2')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization(name='dense2_bn')\n",
    "        self.relu2 = tf.keras.layers.ReLU(name='dense2_relu')\n",
    "\n",
    "        self.cls_dense = tf.keras.layers.Dense(256, name='cls_dense')\n",
    "        self.cls_bn = tf.keras.layers.BatchNormalization(, name='cls_bn')\n",
    "        self.cls_relu = tf.keras.layers.ReLU(name='cls_relu')\n",
    "        self.cls = tf.keras.layers.Dense(1, activation='sigmoid', name='cls_out')\n",
    "\n",
    "        self.bbox_dense = tf.keras.layers.Dense(256, name='bbox_dense')\n",
    "        self.bbox_bn = tf.keras.layers.BatchNormalization(name='bbox_bn')\n",
    "        self.bbox_relu = tf.keras.layers.ReLU(name='bbox_relu')\n",
    "        self.bbox_reg = tf.keras.layers.Dense(4, activation='relu', name='bbox_out')\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        super(Classifier, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "        self.test_loss_tracker = tf.keras.metrics.Mean(name='test_loss')\n",
    "    \n",
    "    def Cls_Loss(self, y_true, y_pred):\n",
    "        return tf.losses.BinaryCrossentropy()(y_true, y_pred)\n",
    "\n",
    "    def Reg_Loss(self, y_true, y_pred, indices):\n",
    "        return tf.losses.Huber()(y_true[indices], y_pred[indices])\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        y_cls = y[0]\n",
    "        y_reg = y[1]\n",
    "        indices = tf.not_equal(y_cls, 0)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            cls, bbox_reg, _ = self(x, training=True)\n",
    "            cls_loss = self.Cls_Loss(y_cls, cls)\n",
    "            reg_loss = self.Reg_Loss(y_reg, bbox_reg, indices)\n",
    "            losses = cls_loss + reg_loss \n",
    "            \n",
    "        trainable_vars = self.trainable_variables\n",
    "        grad = tape.gradient(losses, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(grad, trainable_vars))\n",
    "        self.loss_tracker.update_state(losses)\n",
    "        return {'classifier_loss': self.loss_tracker.result()}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_cls = y[0]\n",
    "        y_reg = y[1]\n",
    "        indices = tf.not_equal(y_cls, 0)\n",
    "\n",
    "        cls, bbox_reg, _ = self(x, training=False)\n",
    "        cls_loss = self.Cls_Loss(y_cls, cls)\n",
    "        reg_loss = self.Reg_Loss(y_reg, bbox_reg, indices)\n",
    "        losses = cls_loss + reg_loss \n",
    "\n",
    "        self.test_loss_tracker.update_state(losses)\n",
    "        return {'classifier_loss_val': self.test_loss_tracker.result()}\n",
    "\n",
    "    \n",
    "    def bbox_regression(self, bbox, nmses):\n",
    "        tx = (bbox[:, :, 0] - nmses[:, :, 0]) / nmses[:, :, 2]\n",
    "        ty = (bbox[:, :, 1] - nmses[:, :, 1]) / nmses[:, :, 3]\n",
    "        tw = tf.math.log(tf.maximum(bbox[:, :, 2], np.finfo(np.float64).eps) / nmses[:, :, 2])\n",
    "        th = tf.math.log(tf.maximum(bbox[:, :, 3], np.finfo(np.float64).eps) / nmses[:, :, 3])\n",
    "        return tf.stack([tx, ty, tw, th], -1)\n",
    "\n",
    "    @staticmethod\n",
    "    def inverse_bbox_regression(bbox, nmses):\n",
    "        gx = nmses[:, :, 2] * bbox[:, :, 0] + nmses[:, :, 0]\n",
    "        gy = nmses[:, :, 3] * bbox[:, :, 1] + nmses[:, :, 1]\n",
    "        gw = nmses[:, :, 2] * tf.exp(bbox[:, :, 2])\n",
    "        gh = nmses[:, :, 3] * tf.exp(bbox[:, :, 3])\n",
    "        return tf.stack([gx, gy, gw, gh], -1)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        rois, nms = inputs\n",
    "\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.conv)(rois)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.bn)(cls_x)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.relu)(cls_x)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.flatten)(cls_x)\n",
    "\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.dense1)(cls_x)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.bn1)(cls_x)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.relu1)(cls_x)\n",
    "        \n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.dense2)(cls_x)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.bn2)(cls_x)\n",
    "        feature_vector = tf.keras.layers.TimeDistributed(self.relu2)(cls_x)\n",
    "\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.cls_dense)(feature_vector)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.cls_bn)(cls_x)\n",
    "        cls_x = tf.keras.layers.TimeDistributed(self.cls_relu)(cls_x)\n",
    "\n",
    "        bbox_x = tf.keras.layers.TimeDistributed(self.bbox_dense)(feature_vector)\n",
    "        bbox_x = tf.keras.layers.TimeDistributed(self.bbox_bn)(bbox_x)\n",
    "        bbox_x = tf.keras.layers.TimeDistributed(self.bbox_relu)(bbox_x)\n",
    "\n",
    "        clss = tf.keras.layers.TimeDistributed(self.cls)(cls_x)\n",
    "        bbox = tf.keras.layers.TimeDistributed(self.bbox_reg)(bbox_x)\n",
    "        bbox_reg = self.bbox_regression(bbox, nms)\n",
    "\n",
    "        return clss, bbox_reg, nms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-terminology",
   "metadata": {},
   "source": [
    "ClassifierëŠ” ê° ì´ë¯¸ì§€ë‹¹ 128ê°œì˜ roiê°€ ì¡´ì¬ í•˜ê¸° ë•Œë¬¸ì— TimeDistributedë¥¼ í†µí•˜ì—¬ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "ì „ì²´ ì ì¸ êµ¬ì¡°ëŠ” ë…¼ë¬¸ì„ ë”°ë¥´ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ê³¼ batch normì„ ì¶”ê°€í•˜ì—¬ í›ˆë ¨ì— ì•ˆì •ì„±ì„ ì£¼ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Faster_RCNN(tf.keras.models.Model):\n",
    "    def __init__(self, img_size, anchor_boxes, k, n_sample, backbone, rpn_lambda, pool_size, **kwargs):\n",
    "        super(Faster_RCNN, self).__init__(*kwargs)\n",
    "        self.img_size = img_size\n",
    "        self.anchor_boxes = anchor_boxes\n",
    "        self.k = k\n",
    "        self.n_sample = n_sample\n",
    "        self.backbone = backbone\n",
    "        self.rpn_lambda = rpn_lambda\n",
    "        self.pool_size = pool_size\n",
    "        self.n_train_pre_nms = 6000\n",
    "        self.n_train_post_nms = 1000\n",
    "        self.n_test_pre_nms = 3000\n",
    "        self.n_test_post_nms = 128\n",
    "        self.iou_threshold = 0.7\n",
    "\n",
    "        self.rpn = RPN(\n",
    "            img_size= self.img_size, \n",
    "            anchor_boxes=self.anchor_boxes, \n",
    "            k=self.k, \n",
    "            n_sample=self.n_sample, \n",
    "            backbone=self.backbone, \n",
    "            rpn_lambda=self.rpn_lambda,\n",
    "            name='rpn'\n",
    "            )\n",
    "        self.get_candidate = get_candidate_layer(name='get_candidate')\n",
    "        self.get_nms = NMS(iou_threshold=self.iou_threshold, name='get_nms')\n",
    "        self.roipool = RoIpool(pool_size=self.pool_size, name='roipool')\n",
    "        self.classifier = Classifier(name='classifier')\n",
    "\n",
    "    def compile(self, rpn_optimizer, classifier_optimizer):\n",
    "        self.rpn.compile(optimizer=rpn_optimizer)\n",
    "        self.classifier.compile(optimizer=classifier_optimizer)\n",
    "        super(Faster_RCNN, self).compile()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        scores, rps, feature_map = self.rpn(inputs)\n",
    "        rps = self.rpn.inverse_bbox_regression(rps)\n",
    "        candidate_area, scores = self.get_candidate((scores, rps, self.n_test_pre_nms))\n",
    "        nms = self.get_nms((candidate_area, scores, self.n_test_post_nms))\n",
    "        rois = self.roipool((feature_map, nms))\n",
    "        cls, bbox_reg, nms = self.classifier((rois, nms))\n",
    "        predict = self.classifier.inverse_bbox_regression(bbox_reg, nms)\n",
    "        return cls, predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-first",
   "metadata": {},
   "source": [
    "rpn, roipooling, nms, classifierë¥¼ í•©ì¹œ Faster_RCNNì„ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "ë…¼ë¬¸ì—ì„œ end-to-endë¼ê³  í•˜ì˜€ëŠ”ë° training stageê°€ 4ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ì ¸ ìˆì–´ train_stepì„ ë”°ë¡œ êµ¬í˜„í•˜ì—¬ í›ˆë ¨ë˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frcnn_train_step(model, train_dataset, train_stage, epochs=1, valid_dataset=None, change_lr=False, rpn_lr=None, cls_lr=None):\n",
    "    if change_lr:\n",
    "        if rpn_lr:\n",
    "            tf.keras.backend.set_value(model.rpn.optimizer.learning_rate, rpn_lr)\n",
    "        if cls_lr:\n",
    "            tf.keras.backend.set_value(model.classifier.optimizer.learning_rate, cls_lr)\n",
    "\n",
    "    if train_stage == 1:\n",
    "        print('Train RPNs \\n')\n",
    "        model.rpn.trainable = True\n",
    "        model.classifier.trainable = False\n",
    "    elif train_stage == 2:\n",
    "        print('Train Fast R-CNN using the proposals from RPNs \\n')\n",
    "        model.rpn.trainable = False\n",
    "        model.rpn.base_model.trainable = True\n",
    "        model.classifier.trainable = True\n",
    "    elif train_stage == 3:\n",
    "        print('Fix the shared convolutional layers and fine-tune unique layers to RPN \\n')\n",
    "        model.rpn.trainable = True\n",
    "        model.rpn.base_model.trainable = False\n",
    "        model.classifier.trainable = False\n",
    "    elif train_stage == 4:\n",
    "        print('Fine-tune unique layers to Fast R-CNN \\n')\n",
    "        model.rpn.trainable = False\n",
    "        model.classifier.trainable = True\n",
    "\n",
    "    max_step = 'Unknown'\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"epoch {epoch+1}/{epochs}\")\n",
    "        display_loss = display(\"Training loss (for one batch) at step 0 : 0\", display_id=True)\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            start = time.time()\n",
    "            y_cls_rpn, y_reg_rpn, gts = y_batch_train\n",
    "            \n",
    "            if train_stage == 1 or train_stage == 3:\n",
    "                result = model.rpn.train_step((x_batch_train, (y_cls_rpn, y_reg_rpn)))\n",
    "                losses = round(float(result['rpn_loss'].numpy()), 5)\n",
    "            else:\n",
    "                scores, rps, feature_map = model.rpn(x_batch_train, training=False)\n",
    "                if train_stage == 2:\n",
    "                    model.rpn.train_step((x_batch_train, (y_cls_rpn, y_reg_rpn)))\n",
    "                rps = model.rpn.inverse_bbox_regression(rps)\n",
    "                candidate_area, scores = model.get_candidate((scores, rps, model.n_train_pre_nms))\n",
    "                nms = model.get_nms((candidate_area, scores, model.n_train_post_nms))\n",
    "                box_labels, cls_labels, nms = classifier_label_generator(nms, gts)\n",
    "                rois = model.roipool((feature_map, nms))\n",
    "                result = model.classifier.train_step(((rois, nms), (cls_labels, box_labels)))\n",
    "                losses = round(float(result['classifier_loss'].numpy()), 5)\n",
    "\n",
    "            display_loss.update(f\"Training loss at step {step}/{max_step} : {losses} - {round(time.time() - start, 4)}sec/step - {time.strftime('%Hh%Mm%Ss', time.gmtime(time.time()-epoch_start))}/epoch\")\n",
    "        max_step = step\n",
    "        display_loss.update(f\"Training loss at step {step}/{max_step} : {losses} - {round(time.time()-start, 4)}sec/step - {time.strftime('%Hh%Mm%Ss', time.gmtime(time.time()-epoch_start))}/epoch\")\n",
    "\n",
    "        if valid_dataset is not None:\n",
    "            display_loss_valid = display(\"validation loss : 0\", display_id=True)\n",
    "            for x_batch_test, y_batch_test in valid_dataset:\n",
    "                y_cls_rpn, y_reg_rpn, gts = y_batch_test\n",
    "\n",
    "                if train_stage == 1 or train_stage == 3:\n",
    "                    result = model.rpn.test_step((x_batch_test, (y_cls_rpn, y_reg_rpn)))\n",
    "                    losses = round(float(result['rpn_loss_val'].numpy()), 5)\n",
    "                else:\n",
    "                    scores, rps, feature_map = model.rpn.predict(x_batch_test)\n",
    "                    rps = model.rpn.inverse_bbox_regression(rps)\n",
    "                    candidate_area, scores = model.get_candidate((scores, rps, model.n_test_pre_nms))\n",
    "                    nms = model.get_nms((candidate_area, scores, model.n_test_post_nms))\n",
    "                    box_labels, cls_labels, nms = classifier_label_generator(nms, gts, valid=True)\n",
    "                    rois = model.roipool((feature_map, nms))\n",
    "                    result = model.classifier.test_step(((rois, nms), (cls_labels, box_labels)))\n",
    "                    losses = round(float(result['classifier_loss_val'].numpy()), 5)\n",
    "                \n",
    "            display_loss_valid.update(f\"validation loss : {losses}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-refrigerator",
   "metadata": {},
   "source": [
    "í›ˆë ¨ ë°ì´í„°ê°€ 3760ê°œ ì—ë‹¤ê°€ ì—°ì†ì ì¸ ì¥ë©´ì´ë¯€ë¡œ augmentationì„ ê±°ì³ë„ ë‹¤ì–‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë˜í•œ test dataì˜ ê²½ìš°ëŠ” ì² ë´‰ê³¼ ê°™ì´ train dataì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°êµ¬ë“¤ë¡œ ì¸í•´ ì‚¬ëŒì´ ê°€ë ¤ì§€ê¸° ë•Œë¬¸ì— ì˜¤ë²„í”¼íŒ…ì„ íŠ¹íˆ ì¡°ì‹¬í•˜ì—¬ í›ˆë ¨ ì‹œí‚¤ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stage = 1\n",
    "epochs = 10\n",
    "\n",
    "frcnn = frcnn_train_step(\n",
    "    model=frcnn, \n",
    "    train_dataset=train_dataset, \n",
    "    valid_dataset=valid_dataset,\n",
    "    train_stage=train_stage,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-apple",
   "metadata": {},
   "source": [
    "ì•„ë˜ëŠ” ê° í›ˆë ¨ stageê°€ ëë‚œë’¤ ê²°ê³¼ë¥¼ ì‹œê°í™” í•œê²ƒì…ë‹ˆë‹¤. \n",
    "ë¹¨ê°„ìƒ‰ ë°•ìŠ¤ê°€ ground truthì´ë©° ì´ì™¸ì— boxê°€ detection ê²°ê³¼ ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-hospital",
   "metadata": {},
   "source": [
    "label setting ë¶€í„° ëª¨ë¸ êµ¬ì¡°, í•˜ì´í¼ íŒŒë¼ë¯¸í„°ê¹Œì§€ ê³ ë¯¼í•˜ë©´ì„œ ì¢‹ì€ ê²½í—˜ì´ ë˜ì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-corrections",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
